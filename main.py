import os, json, re, time, requests
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
from typing import Dict, Any, List, Optional, Literal
from pathlib import Path

from dotenv import load_dotenv
from rich import print as rprint

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from ics import Calendar
from icalendar import Calendar as ICalCalendar
from dateutil.rrule import rrulestr

BASE = Path(__file__).parent

load_dotenv()
FURIOSA_ENDPOINT = os.getenv("FURIOSA_ENDPOINT", "http://127.0.0.1:8000/v1/chat/completions")
FURIOSA_MODEL = None # ÏûêÎèô Ï¥àÍ∏∞Ìôî

SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "").strip()
NOTION_TOKEN = os.getenv("NOTION_TOKEN", "").strip()
NOTION_DB_ID = os.getenv("NOTION_DB_ID", "").strip()
NOTION_TITLE_PROP = "Title"
NOTION_DATE_PROP = "Date"

# Timezone settings
TZ = ZoneInfo("Asia/Seoul")
TZ_LABEL = "KST"

UUID32_RE = re.compile(r"([0-9a-fA-F]{32})")
def _canonical_uuid(s: str) -> str:
    m = UUID32_RE.search(s or "")
    if not m:
        return s
    raw = m.group(1).lower()
    return f"{raw[0:8]}-{raw[8:12]}-{raw[12:16]}-{raw[16:20]}-{raw[20:32]}"

def _normalize_notion_ids():
    global NOTION_DB_ID
    if NOTION_DB_ID:
        canon = _canonical_uuid(NOTION_DB_ID)
        if canon != NOTION_DB_ID:
            rprint(f"[dim]Notion DB ID normalization: {canon}[/dim]")
            NOTION_DB_ID = canon

_normalize_notion_ids()


def _ensure_model():
    global FURIOSA_MODEL
    if FURIOSA_MODEL:
        return FURIOSA_MODEL
    try:
        base_url = FURIOSA_ENDPOINT.split("/v1/")[0] + "/v1/models"
        resp = requests.get(base_url, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            models = []
            if isinstance(data, dict) and "data" in data:
                for m in data["data"]:
                    mid = m.get("id")
                    if mid:
                        models.append(mid)
            if models:
                FURIOSA_MODEL = models[0]
                rprint(f"[dim]Automatically selected model: {FURIOSA_MODEL}[/dim]")
                return FURIOSA_MODEL
    except Exception as e:
        rprint(f"[red]Model auto-selection failed: {e}[/red]")
    FURIOSA_MODEL = "unknown-model"
    return FURIOSA_MODEL

def llm_chat(prompt: str, max_tokens: int = 512, temperature: float = 0.2) -> str:
    model = _ensure_model()
    payload = {
        "model": model,
        "messages": [
            {"role":"system","content":"You are a precise assistant. Always reply in the user's language (Korean if Korean present else English). Maintain concise style and respect JSON format when requested."},
            {"role":"user","content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    try:
        r = requests.post(FURIOSA_ENDPOINT, json=payload, timeout=60)
        if r.status_code >= 400:
            rprint(f"[red]Fail to request LLM {r.status_code}: {r.text[:300]}[/red]")
            raise requests.HTTPError(f"LLM error {r.status_code}")
        data = r.json()
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        rprint(f"[yellow]Fail to request LLM, fallback used: {e}[/yellow]")
        return "{\n  \"action\": \"noop\",\n  \"params\": {\n    \"days\": 1,\n    \"post_to_slack\": false,\n    \"write_to_notion\": false\n  }\n}"

JSON_RE = re.compile(r"\{.*\}", re.DOTALL)
def extract_json(s: str) -> Dict[str, Any]:
    m = JSON_RE.search(s)
    if not m:
        raise ValueError("No JSON found")
    return json.loads(m.group(0))

def fetch_events_from_ical(days: int = 1, start: Optional[datetime] = None) -> List[Dict[str, Any]]:
    cal_text = None

    default_local = BASE / "data" / "basic.ics"
    if default_local.exists():
        source = str(default_local)
    else:
        return []

    def _looks_like_path(s: str) -> bool:
        return s.startswith("./") or s.startswith("../") or s.startswith("/") or (not re.match(r"^[a-zA-Z]+://", s))

    path_candidate = None
    if _looks_like_path(source):
        p = Path(source)
        if not p.exists():
            p2 = (BASE / source).resolve()
            if p2.exists():
                p = p2
        if p.exists():
            path_candidate = p

    if path_candidate is not None and path_candidate.exists():
        try:
            cal_text = path_candidate.read_text(encoding="utf-8")
            rprint(f"[dim]Loaded ICS locally: {path_candidate}[/dim]")
        except Exception as e:
            rprint(f"[yellow]Fail to read local ICS: {e}[/yellow]")
            return []
    else:
        try:
            resp = requests.get(source, timeout=20)
            resp.raise_for_status()
            cal_text = resp.text
            rprint(f"[dim]Fetched ICS remotely: {source}[/dim]")
        except Exception as e:
            rprint(f"[yellow]Fail to fetch ICS: {e}[/yellow]")
            return []

    try:
        ical = ICalCalendar.from_ical(cal_text)
    except Exception as e:
        rprint(f"[yellow]Fail to parse ICS with icalendar: {e}[/yellow]")
        return []

    # Í≤ÄÏÉâ ÏúàÎèÑÏö∞(UTC Í∏∞Ï§Ä)
    utc = timezone.utc
    now_utc = (start or datetime.now(utc)).astimezone(utc)
    end_utc = now_utc + timedelta(days=days)

    def to_aware(dt) -> datetime:
        if isinstance(dt, datetime):
            if dt.tzinfo is None:
                return dt.replace(tzinfo=TZ).astimezone(utc)
            return dt.astimezone(utc)
        return datetime.combine(dt, datetime.min.time(), tzinfo=TZ).astimezone(utc)

    results: List[Dict[str, Any]] = []

    for comp in ical.walk("VEVENT"):
        try:
            dtstart_raw = comp.decoded("DTSTART")
        except Exception:
            continue

        dtend_raw = comp.decoded("DTEND", default=None)
        duration_raw = comp.get("DURATION")

        dtstart = to_aware(dtstart_raw)
        if dtend_raw is not None:
            dtend = to_aware(dtend_raw)
            duration = dtend - dtstart
        elif duration_raw is not None:
            duration = duration_raw.dt if hasattr(duration_raw, "dt") else duration_raw
        else:
            duration = timedelta(hours=1)

        title = str(comp.get("SUMMARY", "No title"))
        location = str(comp.get("LOCATION", ""))

        exdates: set[datetime] = set()
        for ex in comp.get("EXDATE", []):
            try:
                seq = getattr(ex, "dts", None)
                if seq is not None:
                    for d in seq:
                        exdates.add(to_aware(d.dt))
                else:
                    exval = ex.to_ical()
                    pass
            except Exception:
                continue

        rdates: List[datetime] = []
        for rd in comp.get("RDATE", []):
            try:
                seq = getattr(rd, "dts", None)
                if seq is not None:
                    for d in seq:
                        rdates.append(to_aware(d.dt))
            except Exception:
                continue

        rrule_prop = comp.get("RRULE")
        if rrule_prop:
            try:
                rrule_bytes = rrule_prop.to_ical()
                rrule_str = rrule_bytes.decode() if isinstance(rrule_bytes, (bytes, bytearray)) else str(rrule_bytes)
            except Exception:
                if isinstance(rrule_prop, dict):
                    parts = []
                    for k, v in rrule_prop.items():
                        vv = v if isinstance(v, list) else [v]
                        parts.append(f"{k}={','.join(map(str, vv))}")
                    rrule_str = ";".join(parts)
                else:
                    rrule_str = str(rrule_prop)

            rule = rrulestr(rrule_str, dtstart=dtstart.astimezone(utc))

            for occ in rule.between(now_utc, end_utc, inc=True):
                if occ in exdates:
                    continue
                start_utc = occ
                end_utc_inst = start_utc + duration
                if end_utc_inst <= now_utc or start_utc >= end_utc:
                    continue
                results.append({
                    "title": title or "No title",
                    "start": start_utc.astimezone(TZ).strftime("%Y-%m-%d %H:%M"),
                    "end": end_utc_inst.astimezone(TZ).strftime("%Y-%m-%d %H:%M"),
                    "location": location,
                    "tz": TZ_LABEL
                })

            for rdt in rdates:
                start_utc_r = rdt
                end_utc_r = start_utc_r + duration
                if end_utc_r <= now_utc or start_utc_r >= end_utc:
                    continue
                if start_utc_r in exdates:
                    continue
                results.append({
                    "title": title or "No title",
                    "start": start_utc_r.astimezone(TZ).strftime("%Y-%m-%d %H:%M"),
                    "end": end_utc_r.astimezone(TZ).strftime("%Y-%m-%d %H:%M"),
                    "location": location,
                    "tz": TZ_LABEL
                })

        else:
            start_utc = dtstart
            end_utc_inst = dtstart + duration if dtend_raw is None else to_aware(dtend_raw)
            if not (end_utc_inst <= now_utc or start_utc >= end_utc):
                results.append({
                    "title": title or "No title",
                    "start": start_utc.astimezone(TZ).strftime("%Y-%m-%d %H:%M"),
                    "end": end_utc_inst.astimezone(TZ).strftime("%Y-%m-%d %H:%M"),
                    "location": location,
                    "tz": TZ_LABEL
                })

    return sorted(results, key=lambda x: (x["start"], x["end"], x["title"]))

def summarize_events(events: List[Dict[str, Any]], lang: str) -> str:
    if not events:
        return "ÏùºÏ†ïÏù¥ ÏóÜÏäµÎãàÎã§." if lang == "ko" else "No events."
    header = "üìÜ Ïò§Îäò ÏùºÏ†ï ÏöîÏïΩ\n" if lang == "ko" else "üìÜ Schedule Summary\n"
    lines = [header]
    for ev in events:
        start = ev.get("start")  # 'YYYY-MM-DD HH:MM'
        end = ev.get("end")
        title = ev.get("title") or ("(Ï†úÎ™© ÏóÜÏùå)" if lang == "ko" else "(No title)")
        loc = ev.get("location")
        seg = f"{start} - {end} ({TZ_LABEL}) {title}"
        if loc:
            seg += f" @ {loc}"
        lines.append(f"‚Ä¢ {seg}")
    return "\n".join(lines)

def post_to_slack(text: str) -> Dict[str, Any]:
    if not SLACK_WEBHOOK_URL:
        return {"ok": False, "note": "SLACK_WEBHOOK_URL ÎØ∏ÏÑ§Ï†ï"}
    try:
        r = requests.post(SLACK_WEBHOOK_URL, json={"text": text}, timeout=15)
        ok = r.status_code in (200, 204)
        return {"ok": ok, "status": r.status_code, "text": r.text if not ok else "OK"}
    except Exception as e:
        return {"ok": False, "error": str(e)}

def write_to_notion(text: str, lang: str = "ko") -> Dict[str, Any]:
    if not NOTION_TOKEN:
        return {"ok": False, "note": "NOTION_TOKEN ÎØ∏ÏÑ§Ï†ï"}

    url = "https://api.notion.com/v1/pages"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    if lang == "ko":
        title_str = datetime.now().strftime("%Y-%m-%d üìÜ ÏùºÏ†ï ÏöîÏïΩ Î≥¥Í≥†\n")
    else:
        title_str = datetime.now().strftime("%Y-%m-%d üìÜ Schedule Summary\n")
    today_iso = datetime.now().date().isoformat()

    if NOTION_DB_ID:
        properties = {
            NOTION_TITLE_PROP: {"title":[{"text":{"content": title_str}}]},
        }
        if NOTION_DATE_PROP:
            properties[NOTION_DATE_PROP] = {"date": {"start": today_iso}}
        payload = {
            "parent": {"type":"database_id","database_id": NOTION_DB_ID},
            "properties": properties,
            "children": [
                {
                    "object":"block","type":"paragraph",
                    "paragraph":{"rich_text":[{"type":"text","text":{"content": text}}]}
                }
            ]
        }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=20)
        ok = r.status_code in (200, 201)
        return {"ok": ok, "status": r.status_code, "text": r.text if not ok else "OK"}
    except Exception as e:
        return {"ok": False, "error": str(e)}

def plan_from_nl(user_text: str, lang: str) -> Dict[str, Any]:
        if lang == "ko":
                prompt = f"""
Îã§Ïùå Î™ÖÎ†πÏùÑ ÏùΩÍ≥† ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í≥ÑÌöçÏùÑ JSONÏúºÎ°úÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî.
Í∞ÄÎä•Ìïú action:
- "calendar_summary": Ïò§Îäò/ÎÇ¥Ïùº/Ïù¥Î≤àÏ£º ÏùºÏ†ï ÏöîÏïΩ ‚Üí Ïä¨Îûô ‚Üí ÎÖ∏ÏÖò Í∏∞Î°ù
- "noop": ÏïÑÎ¨¥ ÏûëÏóÖ ÏóÜÏùå
Î™ÖÎ†π: "{user_text}"
ÌòïÏãù(Ïù¥ ÌÇ§Îßå):
{{
    "action": "calendar_summary" | "noop",
    "params": {{
        "days": 1,
        "post_to_slack": true,
        "write_to_notion": true
    }}
}}
"""
        else:
                prompt = f"""
Read the command and output ONLY a JSON workflow plan.
Allowed actions:
- "calendar_summary": summarize today's / tomorrow's / week's schedule ‚Üí send to Slack ‚Üí record to Notion
- "noop": do nothing
Command: "{user_text}"
Format (ONLY these keys):
{{
    "action": "calendar_summary" | "noop",
    "params": {{
        "days": 1,
        "post_to_slack": true,
        "write_to_notion": true
    }}
}}
"""
        raw = llm_chat(prompt, max_tokens=300, temperature=0.1)
        try:
                return extract_json(raw)
        except Exception:
                lowered = user_text.lower()
                if lang == "ko":
                        action = "calendar_summary" if ("ÏùºÏ†ï" in user_text or "Ïä§ÏºÄÏ§Ñ" in user_text) else "noop"
                else:
                        action = "calendar_summary" if ("schedule" in lowered or "calendar" in lowered) else "noop"
                return {"action": action, "params": {"days": 1, "post_to_slack": True, "write_to_notion": True}}


from typing import TypedDict

class WFState(TypedDict, total=False):
    user_text: str
    lang: str
    plan: Dict[str, Any]
    events: List[Dict[str, Any]]
    summary: str
    slack_result: Dict[str, Any]
    notion_result: Dict[str, Any]
    error: str

HANGUL_RE = re.compile(r"[\u3131-\u318E\uAC00-\uD7A3]")
def detect_language(text: str) -> str:
    return "ko" if HANGUL_RE.search(text) else "en"

def node_plan(state: WFState) -> WFState:
    lang = detect_language(state["user_text"])
    plan = plan_from_nl(state["user_text"], lang)
    rprint(f"[dim]Plan(lang={lang}) ‚Üí {plan}[/dim]")
    return {"plan": plan, "lang": lang}

def node_calendar(state: WFState) -> WFState:
    p = state.get("plan", {})
    params = p.get("params", {})
    days = int(params.get("days", 1))
    events = fetch_events_from_ical(days=days)
    if not events and days == 1:
        expanded = fetch_events_from_ical(days=30)
        if expanded:
            state['plan']['params']['days'] = 30
            events = expanded
    return {"events": events}

def node_summarize(state: WFState) -> WFState:
    ev = state.get("events", [])
    lang = state.get("lang", "ko")
    summary = "ÏùºÏ†ïÏù¥ ÏóÜÏäµÎãàÎã§." if lang == "ko" else "No events."
    if ev:
        summary = summarize_events(ev, lang)
    return {"summary": summary}

def node_slack(state: WFState) -> WFState:
    res = post_to_slack(state.get("summary",""))
    return {"slack_result": res}

def node_notion(state: WFState) -> WFState:
    lang = state.get("lang", "ko")
    res = write_to_notion(state.get("summary",""), lang=lang)
    return {"notion_result": res}

def should_do_calendar(state: WFState) -> Literal["calendar","end"]:
    if state.get("plan", {}).get("action") == "calendar_summary":
        return "calendar"
    return "end"

def should_post_slack(state: WFState) -> Literal["slack","skip_slack"]:
    if state.get("plan", {}).get("params", {}).get("post_to_slack", False):
        return "slack"
    return "skip_slack"

def should_write_notion(state: WFState) -> Literal["notion","end"]:
    if state.get("plan", {}).get("params", {}).get("write_to_notion", False):
        return "notion"
    return "end"

# Build graph
graph = StateGraph(WFState)
graph.add_node("plan_node", node_plan)
graph.add_node("calendar", node_calendar)
graph.add_node("summarize", node_summarize)
graph.add_node("slack", node_slack)
graph.add_node("notion", node_notion)

graph.set_entry_point("plan_node")
graph.add_conditional_edges("plan_node", should_do_calendar, {"calendar":"calendar", "end": END})
graph.add_edge("calendar", "summarize")
graph.add_conditional_edges("summarize", should_post_slack, {"slack":"slack", "skip_slack":"notion"})
graph.add_conditional_edges("slack", should_write_notion, {"notion":"notion", "end": END})
graph.add_edge("notion", END)

memory = MemorySaver()
app_graph = graph.compile(checkpointer=memory)

def run_once(nl_command: str):
    """
    Execute one-shot workflow from a natural language command.
    Example:
      run_once("ÎÇ¥ ÏùºÏ†ï ÏöîÏïΩÌï¥ÏÑú Ïä¨ÎûôÏóê Î≥¥ÎÇ¥Í≥† ÎÖ∏ÏÖòÏóêÎèÑ Í∏∞Î°ùÌï¥")
    """
    state = {"user_text": nl_command}
    config = {"configurable": {"thread_id": "main"}}
    final = app_graph.invoke(state, config=config)
    rprint("\n[bold cyan]=== Í≤∞Í≥º ===[/bold cyan]")
    rprint({"plan": final.get("plan"),
            "events_count": len(final.get("events", [])),
            "slack": final.get("slack_result"),
            "notion": final.get("notion_result")})
    lang = final.get("lang", "ko")
    header = "ÏöîÏïΩ" if lang == "ko" else "Summary"
    none_text = "(ÏóÜÏùå)" if lang == "ko" else "(none)"
    rprint(f"\n[bold]{header}:[/bold]\n" + final.get("summary", none_text))

if __name__ == "__main__":
    import subprocess

    def get_available_npu():
        try:
            # Run 'furiosa-smi info'
            result = subprocess.run(["furiosa-smi", "info"], capture_output=True, text=True)

            # Example match: "npu1" ‚Üí group(1) = "1"
            match = re.search(r"\bnpu(\d+)\b", result.stdout)
            if match:
                npu_id = match.group(1)
                return f"npu:{npu_id}"

        except Exception as e:
            rprint(f"[red]Failed to detect NPU automatically: {e}[/red]")

        return "npu:0"

    def is_llm_ready():
        try:
            resp = requests.get("http://127.0.0.1:8000/v1/models", timeout=2)
            return resp.status_code == 200
        except Exception:
            return False

    npu_device = get_available_npu()
    llm_proc = None
    if not is_llm_ready():
        rprint("[yellow]LLM server is not running. Starting it automatically...[/yellow]")
        llm_cmd = [
            "furiosa-llm", "serve", "furiosa-ai/Llama-3.1-8B-Instruct-FP8", "--devices", npu_device, "--port", "8000"
        ]
        llm_proc = subprocess.Popen(llm_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        # Wait for server to be ready
        for _ in range(60):
            if is_llm_ready():
                rprint("[green]LLM server is ready![/green]")
                break
            time.sleep(1)
        else:
            rprint("[red]LLM server did not start within 60 seconds. Exiting.[/red]")
            if llm_proc:
                llm_proc.terminate()
            exit(1)

    try:
        rprint("[green]AI Workflow Automator (LangGraph) ‚Äì Local Demo[/green]")
        rprint("Ïòà) Ïò§Îäò ÏùºÏ†ï ÏöîÏïΩÌï¥ÏÑú Ïä¨ÎûôÏóê Î≥¥ÎÇ¥Í≥† ÎÖ∏ÏÖòÏóêÎèÑ Í∏∞Î°ùÌï¥ / e.g. Summarize today‚Äôs schedule and send it to Slack and Notion.")
        text = input("\nÎ™ÖÎ†π(Command) > ").strip()
        if not text:
            text = "Ïò§Îäò ÏùºÏ†ï ÏöîÏïΩÌï¥ÏÑú Ïä¨ÎûôÏóêÎèÑ Î≥¥ÎÇ¥Í≥† ÎÖ∏ÏÖòÏóêÎèÑ Í∏∞Î°ùÌï¥"
        run_once(text)
    finally:
        if llm_proc:
            llm_proc.terminate()